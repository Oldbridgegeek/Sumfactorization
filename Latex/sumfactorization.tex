\documentclass[a4paper,12pt]{report}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[mathscr]{euscript}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{amssymb}


\begin{document}
In 2-D\\
Assume we have a solution $u \in V$ in the following form:
\begin{equation}
u(x,y)=\sum_{h=1}^{n^2} \psi_h (x,y) u_h,\ u_h \in \mathbb{R} \ \forall h =1,...\ ,n^2=:N
\end{equation}
where each $\psi_h$ is a shape function. These shape functions are given by a tensor product of one dimensional polynomials. The transformation given by 
\begin{equation}
h=j(n-1)+i,\ i<n \leftrightarrow (i,j)
\end{equation}
gives us an alternative representation of u:
\begin{equation}
u(x,y)=\sum_{i=1}^n \sum_{j=1}^n \varphi_i(x) \varphi_j(y) u_{ij}, \ i,j =1,...\ n
\end{equation}
Let $q=(q_1,...\ ,q_n)^T$ be quadrature points of a quadrature rule in 1-D with corresponding weights $w=(w_1,...\ ,w_n)^T$. We can get a two-dimensional rule by using the tensor product and obtain points $(\bm{q}_1,...\ ,\bm{q}_N)$ and weights $(\bm{w}_1,...\ ,\bm{w}_N)$ with $\bm{q}_h=(q_i,q_j)$ and $\bm{w}_h=w_i w_j$.\\ \\
First, we want to evaluate $u$ at every quadrature point. This can be done by a matrix vector multiplication:
\begin{equation}
\begin{pmatrix}
\psi_1(\bm{q}_1) & \hdots & \psi_N(\bm{q}_1) \\
\vdots & \ddots & \vdots \\
\psi_1(\bm{q}_N) & \hdots & \psi_N(\bm{q}_N)
\end{pmatrix}
\begin{pmatrix}
u_1 \\
\vdots \\
u_N
\end{pmatrix}
=
\begin{pmatrix}
u(\bm{q}_1) \\
\vdots \\
u(\bm{q}_N)
\end{pmatrix}
\end{equation}
The matrix can be written as a tensor product of two (in this case even identical) matrices:
\begin{equation}
\mathcal{N}^T \otimes \mathcal{N}^T =
\begin{pmatrix}
\varphi_1(q_1) & \hdots & \varphi_n(q_1) \\
\vdots & \ddots & \vdots \\
\varphi_1(q_n) & \hdots & \varphi_n(q_n)
\end{pmatrix}
\otimes
\begin{pmatrix}
\varphi_1(q_1) & \hdots & \varphi_n(q_1) \\
\vdots & \ddots & \vdots \\
\varphi_1(q_n) & \hdots & \varphi_n(q_n)
\end{pmatrix}
\end{equation}
Let $(\mathcal{U}_{ij}) \in \mathbb{R}^{n \times n}$ be the matrix filled with the coefficients $u_{ij}$. Using sum factorization we can multiply these matrices instead of using the formula in (4) which gives us
\begin{equation}
\mathcal{N}^T \mathcal{U} \mathcal{N} = \bar{\mathcal{U}}, \ \ \ \ \bar{\mathcal{U}}_{ij} = u(q_i,q_j)
\end{equation}
The quadrature weights and the summation of the entries can also be done in a efficient way when the tensor product is exploited. Multiplying $\bar{\mathcal{U}}$ with quadrature weights $w$ from the right will sum each weighted column, multyplying $w^T$ from the left will do the rest. The full operation therefore reads
\begin{equation}
\int \int u(x,y) dx \ dy \approx w^T \mathcal{N}^T \mathcal{U} \mathcal{N} w
\end{equation} 
So far, we have only integrated $u$, but we can also test it with an ansatz function without having to change much. Consider some function $f(x,y)=g(x)h(y)$. Since $f$ is seperable, we only have to change the last step. Instead of only multiplying weigths, we will also multiply with the ansatz function evaluated at the respective quadrature point: 
\[\bar{w}^x=(w_1 g(q_1), ... \ ,w_n g(q_n))^T \ \ \ \ \ \ \ \bar{w}^y=(w_1 h(q_1), ... \ ,w_n h(q_n))^T\]
\begin{equation}
\int \int u(x,y) f(x,y) dx \ dy \approx (\bar{w}^x)^T \mathcal{N}^T \mathcal{U} \mathcal{N} \bar{w}^y
\end{equation}

If we have a whole set of ansatz-functions, which are derived from a tensor product of identical one-dimensional ansatz-functions, we can multiply matrices instead of vectors in the last step. This will complete our vmult-operation. We define

\[W=
\begin{pmatrix}
w1 \varphi_1(q_1) & \hdots & w_n \varphi_1(q_n) \\
\vdots & \ddots & \vdots \\
w1 \varphi_n(q_1) & \hdots & w_n \varphi_n(q_n)
\end{pmatrix}
\]
Since the same $\mathcal{W} \mathcal{N}^T$ is used on both sides, this operation can be done by only three matrix multiplications. We will use a multiplication function, which is also able to multiply by the transposed. This will save us the cost of actually transposing a matrix and has no drawbacks. Final formula:
\begin{equation}
\mathcal{V} = \mathcal{W} \mathcal{N}^T \mathcal{U} (\mathcal{W} \mathcal{N}^T)^T
\end{equation}
This formula, when slightly modified, can be used for other bilinearforms aswell. Consider 
$(\nabla u, \nabla v)$.
\begin{align}
(\nabla u, \nabla v) &=(\partial_x u,\partial_x v) + (\partial_y u,\partial_y v) \\
					   &=\sum_{i,j} u_{ij} (\varphi_i'(x)\varphi_j(y),\phi'(x)\phi(y)) \\
					   & \ + \sum_{i,j} u_{ij} (\varphi_i(x)\varphi_j'(y),\phi(x)\phi'(y))
\end{align}
We will introduce the two matrices $\mathcal{W}'$ and $\mathcal{N}'$. These matrices are similar to the matrices above, but instead of evaluating the ansatz-functions they will evaluate their derivative. The resulting formula is given by
\[\mathcal{V}=\mathcal{W}' \mathcal{N}'^T \mathcal{U} (\mathcal{W} \mathcal{N}^T)^T +\mathcal{W} \mathcal{N}^T \mathcal{U} (\mathcal{W}' \mathcal{N}'^T)^T\]
Consider $(-\Delta u, v)=-(\partial_{xx} u,v) + -(\partial_{yy} u,v)$.\\
We introduce $\mathcal{N}''$, which evaluates the second derivatives of the ansatz-functions at the quadrature points. Note that the matrix $\mathcal{W}$ is not modified here. As a result we get 
\[\mathcal{V}=-\mathcal{W} \mathcal{N}''^T \mathcal{U} (\mathcal{W} \mathcal{N}^T)^T -\mathcal{W} \mathcal{N}^T \mathcal{U} (\mathcal{W} \mathcal{N}''^T)^T\]
\end{document}